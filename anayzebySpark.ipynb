{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark to anayze and retrieve data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.functions import *\n",
    "import hdfs\n",
    "from io import BytesIO\n",
    "import fastavro\n",
    "from pyspark.sql.functions import udf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"./spark-2.4.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "hdfs_client = hdfs.InsecureClient(\"http://0.0.0.0:50070\")\n",
    "\n",
    "#################################### Create a Dataframe from .sql files\n",
    "rdd = sc.binaryFiles('hdfs://localhost:9000/data/article_links/master/full/sous-dataset-1/pagelink*.avro') \n",
    "\n",
    "# Parse avro files\n",
    "nodes = rdd.flatMap(lambda args: fastavro.reader(BytesIO(args[1])))\n",
    "\n",
    "# Convert to a resilient distributed dataset (RDD) of rows\n",
    "rows = nodes.map(lambda node: Row(**node))\n",
    "\n",
    "# Create a schema to define the type of each column\n",
    "schema_type=StructType([StructField(\"pl_from\", LongType(), False), StructField(\"pl_title\", StringType(),False)])\n",
    "\n",
    "# Convert to a Spark dataframe\n",
    "df = spark.createDataFrame(rows, schema_type)\n",
    "\n",
    "# Cache data to avoid re-computing everything\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le title we want to find authors writing most\n",
    "TITLE = 'Makémaké'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_as_null(x):\n",
    "    return when(col(x) != \"\", col(x)).otherwise(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithEmptyReplaced = df.withColumn(\"pl_title\", blank_as_null(\"pl_title\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|pl_from|            pl_title|\n",
      "+-------+--------------------+\n",
      "|7661715|'!!!!!!!!_/_Kimi_...|\n",
      "|7942106|'!!!!!!!!_/_Kimi_...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNotNull = dfWithEmptyReplaced.filter(dfWithEmptyReplaced[\"pl_title\"].isNotNull())\n",
    "dfNotNull = dfNotNull.filter((dfNotNull.pl_title != \"'!'\")&(dfNotNull.pl_title != \"'!!'\")&(dfNotNull.pl_title != \"'!!!'\"))\n",
    "dfNotNull.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1512304, 2)\n"
     ]
    }
   ],
   "source": [
    "print((dfNotNull.count(), len(dfNotNull.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We are seeking the number of pages where the title Makémaké is pointed to\n",
      "There are 41804 pages refered to the article\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    counter_tag = dfNotNull.filter(dfNotNull.pl_title.contains(TITLE)).count()\n",
    "    print(\"\\nWe are seeking the number of pages where the title %s is pointed to\" %TITLE)\n",
    "    print(\"There are %d pages refered to the article\" % counter_tag)\n",
    "except:\n",
    "    print(\"\\nNo tag founded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pointed_to_title = dfNotNull.filter(dfNotNull.pl_title.contains(TITLE)).select(\"pl_from\").collect()\n",
    "pages_pointed_to_title =[r.pl_from for r in pages_pointed_to_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11936587, 11936589, 11936591, 11936593, 11936595]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_pointed_to_title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41804"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_pointed_to_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = udf(lambda x: 'pointed' if x in pages_pointed_to_title else x, StringType())\n",
    "k = dfNotNull.withColumn(\"new_pl_from\",F(dfNotNull[\"pl_from\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "| pl_from|           pl_title|new_pl_from|\n",
      "+--------+-------------------+-----------+\n",
      "|11936587|'(136472)_Makémaké'|    pointed|\n",
      "|11936589|'(136472)_Makémaké'|    pointed|\n",
      "|11936591|'(136472)_Makémaké'|    pointed|\n",
      "|11936593|'(136472)_Makémaké'|    pointed|\n",
      "|11936595|'(136472)_Makémaké'|    pointed|\n",
      "|11936597|'(136472)_Makémaké'|    pointed|\n",
      "|11936599|'(136472)_Makémaké'|    pointed|\n",
      "|11936601|'(136472)_Makémaké'|    pointed|\n",
      "|11936603|'(136472)_Makémaké'|    pointed|\n",
      "|11936605|'(136472)_Makémaké'|    pointed|\n",
      "|11936607|'(136472)_Makémaké'|    pointed|\n",
      "|11936609|'(136472)_Makémaké'|    pointed|\n",
      "|11936611|'(136472)_Makémaké'|    pointed|\n",
      "|11936613|'(136472)_Makémaké'|    pointed|\n",
      "|11936615|'(136472)_Makémaké'|    pointed|\n",
      "|11936617|'(136472)_Makémaké'|    pointed|\n",
      "|11936619|'(136472)_Makémaké'|    pointed|\n",
      "|11936621|'(136472)_Makémaké'|    pointed|\n",
      "|11936623|'(136472)_Makémaké'|    pointed|\n",
      "|11936625|'(136472)_Makémaké'|    pointed|\n",
      "+--------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k.filter(k.pl_title.contains(TITLE)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(row):\n",
    "    word1 = \" \".join(re.findall(\"[a-zA-Z]+\", row))\n",
    "    return word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = udf(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = k.withColumn(\"pl_title\", F1(\"pl_title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2= k1.filter(k1['pl_title'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------+\n",
      "| pl_from|          pl_title|new_pl_from|\n",
      "+--------+------------------+-----------+\n",
      "| 7661715|Kimi to Iu Kasetsu|    7661715|\n",
      "| 7942106|Kimi to Iu Kasetsu|    7942106|\n",
      "|  759182|          Fuck You|     759182|\n",
      "| 3923775|          Fuck You|    3923775|\n",
      "|  351979|             album|     351979|\n",
      "|11632317|             album|   11632317|\n",
      "|  903058|              Bang|     903058|\n",
      "| 7671720|           Deladap|    7671720|\n",
      "|  405075|              Huff|     405075|\n",
      "|  574718|                 K|     574718|\n",
      "|  713351|                 K|     713351|\n",
      "| 1155717|                 K|    1155717|\n",
      "| 1327190|                 K|    1327190|\n",
      "| 1783586|                 K|    1783586|\n",
      "| 6114565|                 K|    6114565|\n",
      "| 6117942|                 K|    6117942|\n",
      "| 3959135|         K Records|    3959135|\n",
      "| 4940931|         K Records|    4940931|\n",
      "| 2977893|         Kai Garib|    2977893|\n",
      "|  162147|             Karas|     162147|\n",
      "+--------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_pointed_to_title = k2.filter(k['new_pl_from'] == \"pointed\").select('pl_title').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_pointed_to_title = titles_pointed_to_title.select(col(\"pl_title\").alias(\"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  title|\n",
      "+-------+\n",
      "|Tjelvar|\n",
      "|     XU|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles_pointed_to_title.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### Create a Dataframe from .xmle files\n",
    "rdd1 = sc.binaryFiles('hdfs://localhost:9000/data/article_links/master/full/sous-dataset-1/frwiki*.avro') \n",
    "\n",
    "# Parse avro files\n",
    "nodes1 = rdd1.flatMap(lambda args: fastavro.reader(BytesIO(args[1])))\n",
    "\n",
    "# Convert to a resilient distributed dataset (RDD) of rows\n",
    "rows1 = nodes1.map(lambda node: Row(**node))\n",
    "\n",
    "# Create a schema to define the type of each column\n",
    "schema_type1=StructType([StructField(\"title\", StringType(), False), StructField(\"contributors\", ArrayType(StringType()),False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|        contributors|\n",
      "+--------------------+--------------------+\n",
      "|     Antoine Meillet|[Curry, script de...|\n",
      "|    Algèbre linéaire|[curie.noos.net, ...|\n",
      "|    Algèbre générale|[Youssefsan, Sept...|\n",
      "|       Algorithmique|[Valéry Beaud, ot...|\n",
      "|Politique en Arge...|[Youssefsan, Yous...|\n",
      "|Armée républicain...|[cache7.ihug.com....|\n",
      "|            Autriche|[Alex, Curry, scr...|\n",
      "|            Autriche|[DocteurCosmos, S...|\n",
      "|Arc de triomphe d...|[host213-1-132-15...|\n",
      "|Arc de triomphe d...|[Celette, Gkml, L...|\n",
      "|        Arsène Lupin|[Buzz, Valéry Bea...|\n",
      "|          Algorithme|[Valéry Beaud, sc...|\n",
      "|  Sigles en médecine|[Elisa, Elisa, El...|\n",
      "|Aux couleurs du M...|[Valéry Beaud, sc...|\n",
      "|         Afghanistan|[proxy-4v.club-in...|\n",
      "|         Afghanistan|[Soig, Soig, Soig...|\n",
      "|Algèbre de Boole ...|[Mathic, titan.ac...|\n",
      "|Algèbre de Boole ...|[Erasoft24, Titib...|\n",
      "|       Ada (langage)|[Valéry Beaud, Po...|\n",
      "|            Auvergne|[Rinaldum, Buzz, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to a Spark dataframe\n",
    "df1 = spark.createDataFrame(rows1, schema_type1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(839, 2)\n"
     ]
    }
   ],
   "source": [
    "print((df1.count(), len(df1.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|        contributors|\n",
      "+--------------------+--------------------+\n",
      "|     Antoine Meillet|[Curry, script de...|\n",
      "|    Algèbre linéaire|[curie.noos.net, ...|\n",
      "|    Algèbre générale|[Youssefsan, Sept...|\n",
      "|       Algorithmique|[Valéry Beaud, ot...|\n",
      "|Politique en Arge...|[Youssefsan, Yous...|\n",
      "|Armée républicain...|[cache7.ihug.com....|\n",
      "|            Autriche|[Alex, Curry, scr...|\n",
      "|            Autriche|[DocteurCosmos, S...|\n",
      "|Arc de triomphe d...|[host213-1-132-15...|\n",
      "|Arc de triomphe d...|[Celette, Gkml, L...|\n",
      "|        Arsène Lupin|[Buzz, Valéry Bea...|\n",
      "|          Algorithme|[Valéry Beaud, sc...|\n",
      "|  Sigles en médecine|[Elisa, Elisa, El...|\n",
      "|Aux couleurs du M...|[Valéry Beaud, sc...|\n",
      "|         Afghanistan|[proxy-4v.club-in...|\n",
      "|         Afghanistan|[Soig, Soig, Soig...|\n",
      "|Algèbre de Boole ...|[Mathic, titan.ac...|\n",
      "|Algèbre de Boole ...|[Erasoft24, Titib...|\n",
      "|       Ada (langage)|[Valéry Beaud, Po...|\n",
      "|            Auvergne|[Rinaldum, Buzz, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithEmptyReplaced1 = df1.withColumn(\"title\", blank_as_null(\"title\"))\n",
    "#dfWithEmptyReplaced1.persist()\n",
    "dfWithEmptyReplaced1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|        contributors|\n",
      "+--------------------+--------------------+\n",
      "|     Antoine Meillet|[Curry, script de...|\n",
      "|    Algèbre linéaire|[curie.noos.net, ...|\n",
      "|    Algèbre générale|[Youssefsan, Sept...|\n",
      "|       Algorithmique|[Valéry Beaud, ot...|\n",
      "|Politique en Arge...|[Youssefsan, Yous...|\n",
      "|Armée républicain...|[cache7.ihug.com....|\n",
      "|            Autriche|[Alex, Curry, scr...|\n",
      "|            Autriche|[DocteurCosmos, S...|\n",
      "|Arc de triomphe d...|[host213-1-132-15...|\n",
      "|Arc de triomphe d...|[Celette, Gkml, L...|\n",
      "|        Arsène Lupin|[Buzz, Valéry Bea...|\n",
      "|          Algorithme|[Valéry Beaud, sc...|\n",
      "|  Sigles en médecine|[Elisa, Elisa, El...|\n",
      "|Aux couleurs du M...|[Valéry Beaud, sc...|\n",
      "|         Afghanistan|[proxy-4v.club-in...|\n",
      "|         Afghanistan|[Soig, Soig, Soig...|\n",
      "|Algèbre de Boole ...|[Mathic, titan.ac...|\n",
      "|Algèbre de Boole ...|[Erasoft24, Titib...|\n",
      "|       Ada (langage)|[Valéry Beaud, Po...|\n",
      "|            Auvergne|[Rinaldum, Buzz, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithEmptyReplaced1.filter(dfWithEmptyReplaced1[\"title\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(839, 2)\n"
     ]
    }
   ],
   "source": [
    "print((dfWithEmptyReplaced1.count(), len(dfWithEmptyReplaced1.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group by title and concatenate the lists of contributors\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGroup by title and concatenate the lists of contributors\")\n",
    "\n",
    "df_concat1 = dfWithEmptyReplaced1.rdd.map(lambda x: (x.title,x.contributors)).reduceByKey(lambda x,y:x+y).toDF(['title','contributors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|          title|        contributors|\n",
      "+---------------+--------------------+\n",
      "|Antoine Meillet|[Curry, script de...|\n",
      "+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_concat1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = titles_pointed_to_title.join(df_concat1,\"title\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|            value|count|\n",
      "+-----------------+-----+\n",
      "|       Jonathan71|  168|\n",
      "|           EhOuiH|  107|\n",
      "|        Maxam1392|   93|\n",
      "|            Crom1|   59|\n",
      "|        Ghost dog|   57|\n",
      "|             Ben2|   55|\n",
      "|          Pj44300|   49|\n",
      "|          Salebot|   45|\n",
      "|           MedBot|   38|\n",
      "|       Orthogaffe|   37|\n",
      "|          Aoineko|   36|\n",
      "|     Huguespotter|   35|\n",
      "|           SieBot|   34|\n",
      "|       Goliadkine|   34|\n",
      "|            Aflis|   33|\n",
      "|       Bibliorock|   32|\n",
      "|        Néfermaât|   30|\n",
      "|            Iniți|   29|\n",
      "|            Xqbot|   28|\n",
      "|Loup Solitaire 81|   27|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.select(explode('contributors').alias(\"value\")).groupBy(\"value\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1231.4669077396393 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
